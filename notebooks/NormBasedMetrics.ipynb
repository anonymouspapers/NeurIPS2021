{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm Based Metrics\n",
    "\n",
    "WeightWatcher also provides average norm-based metrics.\n",
    "\n",
    "You can think of these as Product Norm measure of the model complexity $\\mathcal{C}$\n",
    "\n",
    "$$\\mathcal{C}:=\\Vert\\mathbf{W}_{1}\\Vert_{\\circ}\\Vert\\mathbf{W}_{2}\\Vert_{\\circ}\\cdots\\Vert\\mathbf{W}_{L}\\Vert_{\\circ}$$\n",
    "\n",
    "computed as an average over all layers (L),in log-units\n",
    "\n",
    "\n",
    "$$\\log_{10}\\mathcal{C}\\sim\\langle\\log_{10}\\Vert\\mathbf{W}\\Vert_{\\circ}\\rangle:=\\dfrac{1}{L}\\sum\\log_{10}\\Vert\\mathbf{W}\\Vert_{\\circ}$$\n",
    "\n",
    "We need to take an average, not a direct sum, otherwise our metric will spuriously depend on the depth of the network.\n",
    "\n",
    "WeightWatcher computes norm-metrics for each layer, and then reports the average log norm in the summary. These norm metrics include:\n",
    "\n",
    "- the Frobenus Norm, or total mass of the weight matrix\n",
    "\n",
    " $$\\Vert\\mathbf{W}\\Vert_{F}^{2}=(\\sum_{i,j} W_{i,j}^{2})$$\n",
    " \n",
    " Note that this is related to the sum of the eigenvalues $\\lambda_{i}$ of the correlation matrix $\\mathbf{X}=\\mathbf{W}^{T}\\mathbf{W}$\n",
    " \n",
    " $$\\text{norm}:=\\Vert\\mathbf{W}\\Vert^{2}_{F}=\\sum_{i}\\lambda_{i}$$\n",
    "\n",
    "\n",
    " \n",
    "- the Spectral Norm, or the largest eigenvalue $\\lambda_{max}$ of the correlation matrix $\\mathbf{X}$\n",
    "\n",
    " $$\\text{spectralnorm}:=\\Vert\\mathbf{W}\\Vert_{2}^{2}=\\lambda_{max}$$\n",
    "\n",
    "\n",
    "\n",
    "- the $\\alpha$-Shatten Norm, defined by the Power Law fit of the tail of the ESD of $\\mathbf{X}$\n",
    "\n",
    " $$\\text{pnorm}:=\\Vert\\mathbf{W}\\Vert_{2\\alpha}^{2\\alpha}=\\sum_{i} \\lambda_{i}^{\\alpha_{i}}$$\n",
    "\n",
    "\n",
    "\n",
    "- the $\\hat{\\alpha}$ metric, which is a weighted average of $\\alpha$, weighted by the Spectral Norm  of each layer (which approximates the $\\alpha$-Shattem Norm)\n",
    "\n",
    "\n",
    " $$\\text{alpha_weighted}:=\\sum_{i}\\alpha_{i}\\log_{10}\\lambda_{i}$$\n",
    " \n",
    "\n",
    "**Comments:** Unlike $\\alpha$, these Norm-Based metrics are not scale invariant.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results on the Task 1  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'import_ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a226c43c465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkendalltau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUtils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'import_ipynb'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, glob, re\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "import import_ipynb\n",
    "import Utils as U\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies, train_accuracies = U.read_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15,10\n",
    "rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_details(this_task, all_details):\n",
    "    errors = {}\n",
    "    mids = set([100*U.model_num(k) for k in all_details.keys()])\n",
    "\n",
    "    for mid in mids:\n",
    "\n",
    "        fig, axes = plt.subplots(2,3)\n",
    "        subplts = axes.flatten()[::-1].tolist()\n",
    "        model_num = U.model_num(mid)\n",
    "        fig.suptitle(\"{} {}xx models: Average Metric vs. Test Accuracies\".format(this_task, model_num))\n",
    "        fig.tight_layout(pad=5.0)\n",
    "\n",
    "\n",
    "        errors[mid] = {}\n",
    "        for metric in ['lognorm', 'alpha', 'alpha_weighted', 'logspectralnorm', 'logpnorm', 'D']:#'softrank', 'D']:\n",
    "            errors[mid][metric] = {}\n",
    "            x, y = [], []\n",
    "            subplt = subplts.pop()\n",
    "\n",
    "            for model, details in all_details.items():\n",
    "                # remove first and last layers / min_size = 20 really\n",
    "                details = details[details['N']> 10]\n",
    "                details = details[details['M']> 10]\n",
    "\n",
    "                yval = details[metric].dropna().mean()\n",
    "\n",
    "                if yval < 8:\n",
    "                    this_mid = model.replace(\"model_\", '')\n",
    "                    test_accuracy = test_accuracies[this_task][int(this_mid)]\n",
    "\n",
    "                    if int(this_mid) > mid and int(this_mid) < mid+100 :\n",
    "                        x.append(test_accuracy)\n",
    "                        y.append(yval)\n",
    "                        color = U.mid_color(this_task, mid)\n",
    "                        shape = U.mid_shape(this_task, mid)\n",
    "                        subplt.scatter(test_accuracy, yval, color=color, marker=shape)\n",
    "\n",
    "\n",
    "            if len(x)>0:\n",
    "                x = np.array(x).reshape(-1,1)\n",
    "                y = np.array(y).reshape(-1,1)\n",
    "                regr = linear_model.LinearRegression()\n",
    "                regr.fit(x, y)\n",
    "                y_pred = regr.predict(x)\n",
    "                subplt.plot(x, y_pred, color='teal', linewidth=1)\n",
    "\n",
    "                #Table 4\n",
    "                R2 = r2_score(y, y_pred)\n",
    "                MSE = mean_squared_error(y, y_pred)\n",
    "                MAE = mean_absolute_error(y, y_pred)\n",
    "\n",
    "                errors[mid][metric]['MSE'] = MSE\n",
    "                errors[mid][metric]['MAE'] = MAE\n",
    "                errors[mid][metric]['R2'] = R2\n",
    "\n",
    "                tau, p_value = stats.kendalltau(x, y)\n",
    "                errors[mid][metric]['kendal_tau']= tau\n",
    "                errors[mid][metric]['kendal_p_value']= p_value\n",
    "\n",
    "\n",
    "                spman, p_value = spearmanr(x, y)\n",
    "                errors[mid][metric]['spearman_tau']= spman\n",
    "                errors[mid][metric]['spearman_p_value']= p_value  \n",
    "\n",
    "                metname = U.metric_name(metric) \n",
    "                meteqn = U.metric_eqn(metric)\n",
    "\n",
    "                title = metname +\"\\n\"+r\"$R^2$ {:0.2} $\\tau$ {:0.2} SP {:0.2}\".format(R2, tau, spman)\n",
    "                subplt.set_title(title)\n",
    "                \n",
    "                rcParams.update({'font.size': 10})\n",
    "                subplt.set_xlabel(\"Test Accuracy\")\n",
    "                subplt.set_xlabel(\"Avg {} {}\".format(metname, meteqn))\n",
    "                rcParams.update({'font.size': 16})\n",
    "\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 and Task 2 Model Group Results\n",
    "\n",
    "From these results, we see that \n",
    "\n",
    "- the average log Frobenius Norm is mostly correlated with the test accuracy $\\Delta(\\theta)$, but not always\n",
    "- the average log Spectral Norm is strongly anti-correlated with $\\Delta(\\theta)$, but not always\n",
    "\n",
    "- the $\\alpha$ metric is always correlated with $\\Delta(\\theta)$, and while it is not always the strongest correaltion, it is the most consistent\n",
    "\n",
    "Moreover, \n",
    "\n",
    "-in about half the cases, the quality of the $\\alpha$ PL fit itself, the KS-Distance $D$ metric, is correlated with  $\\Delta(\\theta)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 Model Group Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "task1_errors = plot_model_details(this_task, these_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2 Model Group Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details = U.read_details(this_task)\n",
    "task2_errors = plot_model_details(this_task, these_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons Paradox Plot\n",
    "\n",
    "Here are some plots showing how Simpsons Paradox applues to these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simpsons_plot(this_task, all_details, metric='logspectralnorm', eqn=None):\n",
    "    \n",
    "    results = {}\n",
    "    results[this_task] = {'R2':{}, 'Ktau':{}}\n",
    "\n",
    "    mids = set([100*U.model_num(k) for k in all_details.keys()])\n",
    "\n",
    "    all_x, all_y = [], []\n",
    "    for mid in mids:\n",
    "\n",
    "        x, y = [], []\n",
    "\n",
    "        legend=False\n",
    "        for model, details in all_details.items():\n",
    "            # remove first and last layers / min_size = 20 really\n",
    "            details = details[details['N']> 10]\n",
    "            details = details[details['M']> 10]\n",
    "\n",
    "            xval = details[metric].dropna().mean()\n",
    "\n",
    "            if xval < 8:\n",
    "                this_mid = model.replace(\"model_\", '')\n",
    "                test_accuracy = test_accuracies[this_task][int(this_mid)]\n",
    "\n",
    "                if int(this_mid) > mid and int(this_mid) < mid+100 :\n",
    "                    y.append(test_accuracy)\n",
    "                    x.append(xval)\n",
    "                    color = U.mid_color(this_task, mid)\n",
    "                    shape = U.mid_shape(this_task, mid)\n",
    "                    model_num = U.model_num(mid)\n",
    "                    if not legend:  \n",
    "                        plt.scatter(xval, test_accuracy, label=\"{}xx\".format(model_num),  color=color, marker=shape)\n",
    "                        legend = True\n",
    "                    else:\n",
    "                        plt.scatter(xval, test_accuracy,  color=color, marker=shape)\n",
    "\n",
    "\n",
    "        if len(x)>0:\n",
    "            all_x.extend(x)\n",
    "            all_y.extend(y)\n",
    "            \n",
    "            x = np.array(x).reshape(-1,1)\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(x, y)\n",
    "            y_pred = regr.predict(x)\n",
    "            color = U.mid_color(this_task,mid)\n",
    "            model_num = U.model_num(mid)\n",
    "            plt.plot(x, y_pred, color=color, linewidth=1)\n",
    "            \n",
    "            R2 = r2_score(y, y_pred)\n",
    "            tau, p_value = stats.kendalltau(x, y)\n",
    "            print(\"{}xx & {:0.2} & {:0.2} \\\\\\\\\".format(model_num, R2, tau))\n",
    "\n",
    "            key = \"{}xx\".format(model_num)\n",
    "            results[this_task]['R2'][key] = R2\n",
    "            results[this_task]['Ktau'][key] = tau\n",
    "            \n",
    "            \n",
    "    metname = U.metric_name(metric) \n",
    "    if eqn is None:\n",
    "        meteqn = U.metric_eqn(metric)\n",
    "        \n",
    "    x = np.array(all_x).reshape(-1,1)\n",
    "    y = np.array(all_y).reshape(-1,1)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    y_pred = regr.predict(x)\n",
    "    plt.plot(x, y_pred, color=U.AVG_COLOR, linewidth=0.5, linestyle='-')\n",
    "    \n",
    "    \n",
    "    R2 = r2_score(y, y_pred)\n",
    "    tau, p_value = stats.kendalltau(x, y)\n",
    "    print(\"all & {:0.2} & {:0.2} \\\\\\\\\".format(R2, tau))\n",
    "            \n",
    "    taskname = U.taskname(this_task)\n",
    "\n",
    "    #plt.title(r\"{}: Simpsons Plot for {} \".format(this_task,meteqn))\n",
    "    plt.title(r\"Simpsons Plot for {} \".format(taskname,metname),  fontsize=U.TITLE_SIZE)\n",
    "    plt.ylabel(\"Test Accuracy\", fontsize=U.AXIS_SIZE)\n",
    "    plt.xlabel(r\"Avg {} {}\".format(metname,meteqn),  fontsize=U.AXIS_SIZE)\n",
    "    \n",
    "    from matplotlib.ticker import StrMethodFormatter\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}')) # 2 decimal places\n",
    "    plt.tick_params(axis='both', which='major', labelsize=U.TICK_SIZE)\n",
    "    \n",
    "    # add legend path for All plot\n",
    "    import matplotlib.patches as mpatches\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    patch = mpatches.Patch(color=U.AVG_COLOR, label='All', fill=True, alpha=0.4)# hatch='-')\n",
    "    handles.append(patch) \n",
    "    plt.legend(handles=handles, prop={'size': U.LEGEND_SIZE})#, loc='upper right')\n",
    "\n",
    "    filename = \"{}/simpsons-{}-{}.png\".format(U.IMG_DIR, taskname, metname)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 10,10\n",
    "#rcParams.update({'font.size': 26})\n",
    "\n",
    "rcParams['lines.markersize'] = U.MARKER_SIZE\n",
    "\n",
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "logsp_results = simpsons_plot(this_task, these_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details =U.read_details(this_task)\n",
    "results = simpsons_plot(this_task, these_details)\n",
    "logsp_results.update(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "alpha_results =  simpsons_plot(this_task, these_details, metric='alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details = U.read_details(this_task)\n",
    "results = simpsons_plot(this_task, these_details, metric='alpha')\n",
    "alpha_results.update(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "alpha_weighted_results = simpsons_plot(this_task, these_details, metric='alpha_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details = U.read_details(this_task)\n",
    "alpha_weighted_results = simpsons_plot(this_task, these_details, metric='alpha_weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in [\"task1_v4\", \"task2_v1\"]:\n",
    "\n",
    "    for metric, metname in zip([\"R2\", \"Ktau\"], [U.R_SQUARED, U.KENDAL_TAU]):\n",
    "        \n",
    "        metname = U.metric_name(metric) \n",
    "        \n",
    "        #title = \"{} for Regression on Test Accuracies for {} \\n\".format(metname, task)\n",
    "        title = \"{} for Regression on Test Accuracies \\n\".format(metname)\n",
    "        title += U.ALPHA +\" \"+U.AVG_ALPHA_EQN+\" and \"+U.LOG_SPECTRALNORM+\" \"+U.AVG_LOG_SPECTRALNORM_EQN\n",
    "\n",
    "        alpha = alpha_results[task][metric]\n",
    "        logsp = logsp_results[task][metric]\n",
    "\n",
    "        barWidth = 0.25\n",
    "\n",
    "        bars = alpha.keys()\n",
    "        r1 = np.arange(len(bars))\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "\n",
    "        plt.bar(r1, logsp.values(), label=U.AVG_LOG_SPECTRALNORM_EQN, color='green',width=barWidth, edgecolor='white')\n",
    "        plt.bar(r2, alpha.values(), label=U.AVG_ALPHA_EQN, alpha=0.5, color='blue',width=barWidth, edgecolor='white')\n",
    "        plt.xticks([r + barWidth for r in range(len(bars))], bars)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=U.TICK_SIZE)\n",
    "\n",
    "        plt.legend(prop={'size': U.LEGEND_SIZE+4}, loc='upper right')\n",
    "        if metname=='Ktau':\n",
    "            metname = r\"Kemdal-$\\tau$\"\n",
    "        plt.ylabel(metname, fontsize=U.AXIS_SIZE);\n",
    "        #plt.title(title, fontsize=U.TITLE_SIZE)\n",
    "        plt.savefig(\"{}/alpha-spnorm-{}-test-accs-{}\".format(U.IMG_DIR, metric, task))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare log spectral norm to alpha for each model group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics_plot(this_task, all_details):\n",
    "    \"\"\"Compare log spectral norm to alpha for each model group\"\"\"\n",
    "    \n",
    "    results = {'R2':{}, 'Ktau':{}}\n",
    "    \n",
    "    mids = set([100*U.model_num(k) for k in all_details.keys()])\n",
    "\n",
    "    for mid in mids:\n",
    "\n",
    "        x, y = [], []\n",
    "\n",
    "        legend=False\n",
    "        for model, details in all_details.items():\n",
    "            # remove first and last layers / min_size = 20 really\n",
    "            details = details[details['N']> 10]\n",
    "            details = details[details['M']> 10]\n",
    "\n",
    "            xlabel=r\"Avg LogSpectralNorm \"+U.AVG_LOG_SPECTRALNORM_EQN\n",
    "            xval = details['logspectralnorm'].dropna().mean()\n",
    "            \n",
    "            ylabel=\"Avg Alpha \"+U.AVG_ALPHA_EQN\n",
    "            yval = details['alpha'].dropna().mean()\n",
    "                \n",
    "            this_mid = model.replace(\"model_\", '')\n",
    "            if int(this_mid) > mid and int(this_mid) < mid+100 :\n",
    "                x.append(xval)\n",
    "                y.append(yval)\n",
    "\n",
    "                    \n",
    "        if len(x)>0:\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            p = x.argsort()     \n",
    "            x = x[p].reshape(-1,1)\n",
    "            y = y[p].reshape(-1,1)\n",
    "            color = U.mid_color(this_task, mid)\n",
    "            shape = U.mid_shape(this_task, mid)\n",
    "            model_num = U.model_num(mid)\n",
    "            plt.scatter(x, y, label=\"{}xx\".format(model_num), color=color, marker=shape)\n",
    "                  \n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(x, y)\n",
    "            y_pred = regr.predict(x)\n",
    "            plt.plot(x, y_pred,  linewidth=1 ,color=color)\n",
    "            \n",
    "             # table 3\n",
    "            R2 = r2_score(y, y_pred)\n",
    "            tau, p_value = stats.kendalltau(x, y)\n",
    "            print(\"{}xx & {:0.2} & {:0.2} \\\\\\\\\".format(model_num, R2, tau))\n",
    "\n",
    "            key = \"{}xx\".format(model_num)\n",
    "            results['R2'][key] = R2\n",
    "            results['Ktau'][key] = tau\n",
    "        \n",
    "        title =  \"Avg Alpha \"+U.AVG_ALPHA_EQN+\" vs. \\n Avg LogSpectralNorm \"+U.AVG_LOG_SPECTRALNORM_EQN\n",
    "        title =  \"Alpha vs. LogSpectralNorm \"\n",
    "\n",
    "        # removed for paper\n",
    "        plt.title(title, fontsize=U.TITLE_SIZE)\n",
    "        plt.xlabel(xlabel, fontsize=U.AXIS_SIZE)\n",
    "        plt.ylabel(ylabel, fontsize=U.AXIS_SIZE)\n",
    "        plt.legend(prop={'size': U.LEGEND_SIZE})#,loc='upper right')\n",
    "        \n",
    "        plt.tick_params(axis='both', which='major', labelsize=U.TICK_SIZE)\n",
    "                  \n",
    "        taskname = U.taskname(this_task)\n",
    "        model_num = U.model_num(mid)\n",
    "        filename = \"{}/alpha-vs-spnorm-{}-{}xx.png\".format(U.IMG_DIR,taskname,model_num)\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_metrics_plot(this_task, all_details):\n",
    "    \"\"\"Compare log spectral norm to alpha for each model group, all together\n",
    "    Also prints results for appendix table3\"\"\"\n",
    "     \n",
    "    results = {'R2':{}, 'Ktau':{}}\n",
    "    \n",
    "    mids = set([100*U.model_num(k) for k in all_details.keys()])\n",
    "\n",
    "    for mid in mids:\n",
    "\n",
    "        x, y = [], []\n",
    "\n",
    "        legend=False\n",
    "        for model, details in all_details.items():\n",
    "            # remove first and last layers / min_size = 20 really\n",
    "            details = details[details['N']> 10]\n",
    "            details = details[details['M']> 10]\n",
    "\n",
    "            xlabel=r\"Avg LogSpectralNorm \"+U.AVG_LOG_SPECTRALNORM_EQN\n",
    "            xval = details['logspectralnorm'].dropna().mean()\n",
    "            \n",
    "            ylabel=\"Avg Alpha \"+U.AVG_ALPHA_EQN\n",
    "            yval = details['alpha'].dropna().mean()\n",
    "                \n",
    "            this_mid = model.replace(\"model_\", '')\n",
    "            if int(this_mid) > mid and int(this_mid) < mid+100 :\n",
    "                x.append(xval)\n",
    "                y.append(yval)\n",
    "\n",
    "                    \n",
    "        if len(x)>0:\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            p = x.argsort()     \n",
    "            x = x[p].reshape(-1,1)\n",
    "            y = y[p].reshape(-1,1)\n",
    "            color = U.mid_color(this_task, mid)\n",
    "            shape = U.mid_shape(this_task, mid)\n",
    "            model_num = U.model_num(mid)\n",
    "            plt.scatter(x, y, label=\"{}xx\".format(model_num), color=color, marker=shape)\n",
    "                  \n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(x, y)\n",
    "            y_pred = regr.predict(x)\n",
    "            plt.plot(x, y_pred,  linewidth=1 ,color=color)\n",
    "            \n",
    "             # table 3\n",
    "            R2 = r2_score(y, y_pred)\n",
    "            tau, p_value = stats.kendalltau(x, y)\n",
    "            print(\"{}xx & {:0.2} & {:0.2} \\\\\\\\\".format(model_num, R2, tau))\n",
    "\n",
    "            key = \"{}xx\".format(model_num)\n",
    "            results['R2'][key] = R2\n",
    "            results['Ktau'][key] = tau\n",
    "            \n",
    "    results['R2']['AVG'] = np.mean([x for x in results['R2'].values()])\n",
    "    results['Ktau']['AVG'] = np.mean([x for x in results['Ktau'].values()])\n",
    "    print(\"{} & {:0.2} & {:0.2} \\\\\\\\\".format('AVG',  results['R2']['AVG'], results['Ktau']['AVG']))\n",
    "        \n",
    "    title =  \"Avg Alpha \"+U.AVG_ALPHA_EQN+\" vs. \\n Avg LogSpectralNorm \"+U.AVG_LOG_SPECTRALNORM_EQN\n",
    "    title =  \"Alpha vs. LogSpectralNorm \"\n",
    "\n",
    "    # removed for paper\n",
    "    plt.title(title, fontsize=U.TITLE_SIZE)\n",
    "    plt.xlabel(xlabel, fontsize=U.AXIS_SIZE)\n",
    "    plt.ylabel(ylabel, fontsize=U.AXIS_SIZE)\n",
    "    plt.legend(prop={'size': U.LEGEND_SIZE})#,loc='upper right')\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=U.TICK_SIZE)\n",
    "\n",
    "    taskname = U.taskname(this_task)\n",
    "    model_num = U.model_num(mid)\n",
    "    filename = \"{}/alpha-vs-spnorm-{}.png\".format(U.IMG_DIR,taskname)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "task1_results = combine_metrics_plot(this_task, these_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details = U.read_details(this_task)\n",
    "task2_results = combine_metrics_plot(this_task, these_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "this_task = \"task1_v4\"\n",
    "these_details = U.read_details(this_task)\n",
    "compare_metrics_plot(this_task, these_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_task = \"task2_v1\"\n",
    "these_details = U.read_details(this_task)\n",
    "compare_metrics_plot(this_task, these_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melted_df(errors, metric, task=1):\n",
    "    if task==1:\n",
    "        cols = { 0:'0xx', 100:'1xx', 200:'2xx', 500:'5xx', 600:'6xx', 700:'7xx'}\n",
    "    else:\n",
    "        cols = { 200:'2xx', 600:'6xx', 900:'9xx', 1000:'10xx'}        \n",
    "\n",
    "    df = pd.DataFrame(errors)\n",
    "\n",
    "    df = df.rename(columns=cols)\n",
    "    for col in cols.values():\n",
    "        df[col]=df[col].apply(lambda x: x[metric])\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index':'metric'})\n",
    "    mdf = df.melt('metric')\n",
    "    mdf.rename(columns={'variable':'model', 'value':metric}, inplace=True)\n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_kt = melted_df(task1_errors, 'kendal_tau', task=1)\n",
    "df1_r2 = melted_df(task1_errors, 'R2', task=1)\n",
    "df2_kt = melted_df(task2_errors, 'kendal_tau', task=2)\n",
    "df2_r2 = melted_df(task2_errors, 'R2', task=2)\n",
    "table4_df1 = pd.merge(df1_kt,df1_r2)\n",
    "table4_df2 = pd.merge(df2_kt,df2_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_metric(df):\n",
    "    all_R2 = df.groupby('metric')['R2'].apply(np.average).to_frame()\n",
    "    all_Kt = df.groupby('metric')['kendal_tau'].apply(np.average).to_frame()\n",
    "    all_df = all_R2.join(all_Kt)\n",
    "    all_df['model']='AVG'\n",
    "    all_df.reset_index(inplace=True)\n",
    "    return pd.concat([df, all_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table4_df1 = add_avg_metric(table4_df1)\n",
    "table4_df2 = add_avg_metric(table4_df2)\n",
    "table4_dfs= [table4_df1, table4_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metrics = ['logspectralnorm','lognorm', 'alpha', 'D']\n",
    "measures = ['R2', 'kendal_tau']\n",
    "tasks = ['\\TASKONE', '\\TASKTWO']\n",
    "\n",
    "HEADER = \"\"\"\n",
    "\\\\begin{tabular}{|p{1.05in}|c|c|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& \\multicolumn{2}{|c|}{ \\SPECTRALNORM      } \n",
    "& \\multicolumn{2}{|c|}{ \\FROBENIUSNORM     } \n",
    "& \\multicolumn{2}{|c|}{ \\ALPHA             } \n",
    "& \\multicolumn{2}{|c|}{ \\QUALITYOFALPHAFIT } \n",
    "\\\\\\\\\n",
    "\\hline\n",
    "               &  $R^2$ & Kendal-$\\\\tau$ &  $R^2$ & Kendal-$\\\\tau$ &  $R^2$ & Kendal-$\\\\tau$ &  $R^2$ & Kendal-$\\\\tau$  \\\\\\\\\n",
    "\"\"\"\n",
    "    \n",
    "with open('./table4.tex', 'w') as f:\n",
    "\n",
    "    f.write(HEADER)\n",
    "    for df, task in zip(table4_dfs,tasks):\n",
    "        #print(task+\" - ALL & XXX & XXX & XXX & XXX & XXX & XXX & XXX & XXX  \\\\\\\\\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        mids = df.model.unique()\n",
    "                \n",
    "        for mid in mids:\n",
    "            \n",
    "            # bold the largest R2, KTau for each task\n",
    "            R2s, KTs = [], []\n",
    "            for metric in metrics:\n",
    "                R2 = df[(df.model==mid) & (df.metric==metric) ]['R2'].to_numpy()[0]\n",
    "                Ktau = df[(df.model==mid) & (df.metric==metric) ]['kendal_tau'].to_numpy()[0]\n",
    "                R2s.append(R2)\n",
    "                KTs.append(Ktau)\n",
    "            max_r2_id = np.argmax(R2s)\n",
    "            max_kt_id = np.argmax(np.abs(KTs))\n",
    "            \n",
    "            line = \"{} - {} &\".format(task,mid)\n",
    "            for im, metric in enumerate(metrics):\n",
    "                R2 = df[(df.model==mid) & (df.metric==metric) ]['R2'].to_numpy()[0]\n",
    "                Ktau = df[(df.model==mid) & (df.metric==metric) ]['kendal_tau'].to_numpy()[0]\n",
    "                R2_str = \" {:0.2f} \".format(R2)\n",
    "                Ktau_str = \" {:0.2f} \".format(Ktau)\n",
    "                if im==max_r2_id:\n",
    "                    R2_str = \"\\\\textbf{\"+R2_str+\"}\"\n",
    "                if im==max_kt_id:\n",
    "                    Ktau_str = \"\\\\textbf{\"+Ktau_str+\"}\"\n",
    "                    \n",
    "                line = line + R2_str + \" & \"+ Ktau_str+ \" &\"\n",
    "            f.write(line[:-1]+\"\\\\\\\\\\n\")\n",
    "            f.write(\"\\hline\\n\")\n",
    "            line = \"\"\n",
    "\n",
    "        f.write(line)\n",
    "    f.write(\"\\end{tabular}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp table4.tex ../paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat table4.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp ./img/*png ../paper/img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
