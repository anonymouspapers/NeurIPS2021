{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, glob, re\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./img\"\n",
    "RESULTS_DIR = \"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK1_COLORS = {'0xx':'green',\n",
    "                '1xx':'darkorange',\n",
    "                '2xx':'dodgerblue',\n",
    "                '5xx':'blue',\n",
    "                '6xx':'violet',\n",
    "                '7xx':'purple' ,\n",
    "                'all': 'red'}\n",
    "\n",
    "TASK2_COLORS = {'2xx':'royalblue', \n",
    "                '6xx':'cadetblue', \n",
    "                '9xx':'darkorange', \n",
    "                '10xx':'darkmagenta',\n",
    "                'all': 'red'}\n",
    "\n",
    "TASK1_SHAPES = {'0xx':'o',\n",
    "                '1xx':'s',\n",
    "                '2xx':'v',\n",
    "                '5xx':'^',\n",
    "                '6xx':'<',\n",
    "                '7xx':'>' ,\n",
    "                'all': 'd'}\n",
    "\n",
    "TASK2_SHAPES= {'2xx':'o', \n",
    "                '6xx':'s', \n",
    "                '9xx':'v', \n",
    "                '10xx':'^',\n",
    "                'all': 'd'}\n",
    "\n",
    "\n",
    "ALL_COLOR = 'dimgray'\n",
    "AVG_COLOR = ALL_COLOR\n",
    "\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "TITLE_SIZE = 40\n",
    "AXIS_SIZE = 36\n",
    "LEGEND_SIZE = 24\n",
    "TICK_SIZE = 20\n",
    "MARKER_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_LOG_SPECTRALNORM_EQN = r\"$\\langle\\log_{10}\\Vert\\mathbf{W}\\Vert^{2}_{\\infty}\\rangle$\"\n",
    "AVG_ALPHA_EQN = r\"$\\langle\\alpha\\rangle$\"\n",
    "ALPHA_HAT = AVG_ALPHA_WEIGHTED_EQN = r\"$\\hat\\alpha$\"\n",
    "\n",
    "W_DISTANCE_EQN = r\"$\\log_{10}\\langle\\Vert\\mathbf{W}-\\mathbf{W}_{init}\\Vert^{2}_{F}\\rangle$\"\n",
    "QUALITY_FIT_EQN = r\"$\\langle D_{KS}\\rangle$\"\n",
    "\n",
    "AVG_LOG_NORM_EQN = r\"$\\langle\\Vert\\mathbf{W}\\Vert^{2}_{F}\\rangle$\"\n",
    "\n",
    "SHARPNESS_EQN = r\"Sharpness\"\n",
    "SVD_10_EQN = r\"SVD $10%$\"\n",
    "SVD_20_EQN = r\"SVD $20%$\"\n",
    "\n",
    "\n",
    "\n",
    "LOG_SPECTRALNORM= \"LogSpectralNorm\"\n",
    "ALPHA = \"Alpha\"\n",
    "ALPHA_HAT = AVG_ALPHA_WEIGHTED = \"AlphaHat\"\n",
    "NORM = \"LogFrobeniusNorm\"\n",
    "D_ALPHA_FIT = \"LogFrobeniusNorm\"\n",
    "ALPHA_PNORM = \"LogAlphaShattenNorm\"\n",
    "\n",
    "KENDAL_TAU = \"Kendal-tau\"\n",
    "R_SQUARED = \"R-Squared\"\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def taskname(task):\n",
    "    taskname = task\n",
    "    taskname = taskname.replace(\"_v1\",'')\n",
    "    taskname = taskname.replace(\"_v4\",'')\n",
    "    return taskname\n",
    "\n",
    "def metric_name(metric):\n",
    "    name = metric\n",
    "    if metric == \"logspectralnorm\":\n",
    "        name = LOG_SPECTRALNORM\n",
    "    elif metric == 'alpha':\n",
    "        name = ALPHA\n",
    "    elif metric == 'alpha_weighted':\n",
    "        name = ALPHA_HAT\n",
    "    elif metric == 'lognorm':\n",
    "        name = NORM\n",
    "    elif metric == \"D\":\n",
    "        name = D_ALPHA_FIT\n",
    "    elif metric == \"logpnorm\":\n",
    "        name = ALPHA_PNORM\n",
    "    return name\n",
    "\n",
    "def metric_eqn(metric):\n",
    "    eqn = \"\"\n",
    "    if metric == 'logspectralnorm':\n",
    "        eqn = AVG_LOG_SPECTRALNORM_EQN\n",
    "    elif metric == 'alpha':\n",
    "        eqn = AVG_ALPHA_EQN\n",
    "    elif metric == 'alpha_weighted':\n",
    "        eqn = AVG_ALPHA_WEIGHTED_EQN\n",
    "    elif metric == \"D\":\n",
    "        eqn = QUALITY_FIT_EQN\n",
    "    elif metric == 'lognorm':\n",
    "        eqn = AVG_LOG_NORM_EQN\n",
    "    elif metric == \"sharpness\":\n",
    "        eqn = SHARPNESS_EQN\n",
    "    elif metric == \"svd10\":\n",
    "        eqn = SVD_10_EQN\n",
    "    elif metric == \"svd20\":\n",
    "        eqn = SVD_20_EQN\n",
    "    elif metric == \"W_distance\":\n",
    "        eqn = W_DISTANCE_EQN\n",
    "        \n",
    "    return eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_num(k):\n",
    "    return int(int(k)/100)\n",
    "\n",
    "def model_group(k):\n",
    "    return \"{}xx\".format(model_num(k))\n",
    "\n",
    "def mid_color(task,mid):\n",
    "    color = None\n",
    "    key = \"{}xx\".format(model_num(mid))\n",
    "    \n",
    "    if task=='task1_v4':\n",
    "        color = TASK1_COLORS[key]\n",
    "    else:\n",
    "        color = TASK2_COLORS[key]\n",
    "    return color\n",
    "\n",
    "\n",
    "\n",
    "def mid_shape(task,mid):\n",
    "    shape = None\n",
    "    key = \"{}xx\".format(model_num(mid))\n",
    "    \n",
    "    if task=='task1_v4':\n",
    "        shape = TASK1_SHAPES[key]\n",
    "    else:\n",
    "        shape = TASK2_SHAPES[key]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_accuracies():\n",
    "    \"\"\"Read the test accuracies from the public reference data model configs\"\"\"\n",
    "    test_accuracies = {}\n",
    "    train_accuracies = {}\n",
    "    for task in [\"task1_v4\", \"task2_v1\"]:\n",
    "        filename = \"../public_data/reference_data/{}/model_configs.json\".format(task)\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        test_accuracies[task]= {}\n",
    "        train_accuracies[task]= {}\n",
    "        for k, v in data.items():\n",
    "            test_accuracies[task][int(k)]=v['metrics']['test_acc']\n",
    "            train_accuracies[task][int(k)]=v['metrics']['train_acc']\n",
    "            \n",
    "    return test_accuracies, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_details(this_task):\n",
    "    \"\"\"Read the WeightWatcher details dataframe for all models in this task\"\"\"\n",
    "\n",
    "    path = r\"../results/{}/model_*\".format(this_task)\n",
    "    regex = re.compile(path, re.IGNORECASE)\n",
    "\n",
    "    all_details = {}\n",
    "    for fullname in glob.glob(path): \n",
    "        mid = re.sub(path,'', fullname).replace('.csv', '')\n",
    "        all_details[mid] = pd.read_csv(fullname)\n",
    "\n",
    "    num = len(all_details)\n",
    "    print(\"Read {} details dataframes for {}\".format(num, this_task))\n",
    "    assert(num>0)\n",
    "    \n",
    "    return all_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_task_metrics(task, out_dir):\n",
    "    \"\"\"Read metric values (i.e. W-distance) in the task.predict file, as computed by the context ingestion program\"\"\"\n",
    "    # collect all outputs\n",
    "    filename = \"{}/{}.predict\".format(out_dir,task)\n",
    "    #print(filename)\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = {}\n",
    "    for k, v in data.items():\n",
    "        ik = int(k.replace(\"model_\",\"\"))\n",
    "        metrics[int(ik)]=v\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_modelgroup_accuracies():\n",
    "    \"\"\"get arrays of accuracies for each model group\"\"\"\n",
    "    test_group_accuracies = {}\n",
    "    train_group_accuracies = {}\n",
    "    \n",
    "    for task in [\"task1_v4\", \"task2_v1\"]:\n",
    "\n",
    "        filename = \"../public_data/reference_data/{}/model_configs.json\".format(task)\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        test_group_accuracies[task]= {}\n",
    "        train_group_accuracies[task]= {}\n",
    "\n",
    "        for k, v in data.items():\n",
    "            # form model group list\n",
    "            test_group_accuracies[task][model_num(k)]=[]\n",
    "            train_group_accuracies[task][model_num(k)]=[]\n",
    "\n",
    "        for k, v in data.items():\n",
    "            test_group_accuracies[task][model_num(k)].append([int(k),v['metrics']['test_acc']])\n",
    "            train_group_accuracies[task][model_num(k)].append([int(k),v['metrics']['train_acc']])\n",
    "\n",
    "        \n",
    "    return test_group_accuracies, train_group_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
